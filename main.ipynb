{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45d947b353e24e20ad85dbe6052119cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5ad8413f4b3741328c6d4f9ff1da3b59",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f92a1d0c9c743d896816a3a7304c161",
              "IPY_MODEL_9274f97b42b041d2810bb1e508dd723f"
            ]
          }
        },
        "5ad8413f4b3741328c6d4f9ff1da3b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f92a1d0c9c743d896816a3a7304c161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_168c1f7b53e04e7bbc785f709e80a88f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05cbccfafd8445b394b55bf50bc575e5"
          }
        },
        "9274f97b42b041d2810bb1e508dd723f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9bf4c9bb11be432197aedee9541698dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "2it [00:03,  1.99s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85fe45ba6c72416b8df9231cfc063e35"
          }
        },
        "168c1f7b53e04e7bbc785f709e80a88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05cbccfafd8445b394b55bf50bc575e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9bf4c9bb11be432197aedee9541698dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85fe45ba6c72416b8df9231cfc063e35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enoVKA3K9Aun",
        "colab_type": "text"
      },
      "source": [
        "# Data and Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h85kpXt0a3P8",
        "colab_type": "code",
        "outputId": "83f59b97-7f34-4287-aeb5-82383b77f0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !ln -s "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TURGzsGnbSsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp /content/drive/'My Drive'/project/project/trim/sample_submission.csv .\n",
        "# !cp /content/drive/'My Drive'/project/trim/orig/export.pkl ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blZJ2szjcfGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ln -s /content/drive/'My Drive'/project/trim/test/ /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qw7eCLkgFDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !awk '{sub(/[^,]*/,\"\");sub(/,/,\"\")} 1' sample_submission.csv > audio.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB5qe4ZvWzK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.vision import *\n",
        "from tqdm import tqdm_notebook\n",
        "import IPython\n",
        "import IPython.display\n",
        "import PIL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV5lAMV-TCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = Path('/content')\n",
        "FOLDER = 'aud' #for audio files\n",
        "SOURCE = ROOT/FOLDER\n",
        "LIST = ROOT/'head.csv'\n",
        "CATMODEL = 'export.pkl' #model for categorization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8YsapIa6vTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "40ee9305-efea-4a8d-eedd-283a6b1d9f83"
      },
      "source": [
        "!wget -O export.pkl 'https://drive.google.com/uc?id=1GiUQOVgGckM95dT30MvxxPPXMVkwlFwk&export=download' "
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-28 02:45:19--  https://drive.google.com/uc?id=1GiUQOVgGckM95dT30MvxxPPXMVkwlFwk&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.132.102, 74.125.132.138, 74.125.132.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.132.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/189nji525ait7n4bp6i98n4et2j1b00u/1580176800000/07199574015187888320/*/1GiUQOVgGckM95dT30MvxxPPXMVkwlFwk?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-01-28 02:45:21--  https://doc-0g-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/189nji525ait7n4bp6i98n4et2j1b00u/1580176800000/07199574015187888320/*/1GiUQOVgGckM95dT30MvxxPPXMVkwlFwk?e=download\n",
            "Resolving doc-0g-ac-docs.googleusercontent.com (doc-0g-ac-docs.googleusercontent.com)... 172.217.214.132, 2607:f8b0:4001:c05::84\n",
            "Connecting to doc-0g-ac-docs.googleusercontent.com (doc-0g-ac-docs.googleusercontent.com)|172.217.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: â€˜export.pklâ€™\n",
            "\n",
            "\rexport.pkl              [<=>                 ]       0  --.-KB/s               \rexport.pkl              [ <=>                ]  44.54M   223MB/s               \rexport.pkl              [  <=>               ]  44.92M   224MB/s    in 0.2s    \n",
            "\n",
            "2020-01-28 02:45:21 (224 MB/s) - â€˜export.pklâ€™ saved [47104116]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jAQuzI77v70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "308f9720-7e66-412a-fd8d-bc3fc536f9d9"
      },
      "source": [
        "#sample audio\n",
        "!wget https://sampleswap.org/samples-ghost/VOCALS%20and%20SPOKEN%20WORD/Commercials%20and%20Radio/3207[kb]1941-newsreel-co_ed_warrior_women.aif.mp3\n",
        "!sox '3207[kb]1941-newsreel-co_ed_warrior_women.aif.mp3' aud.wav"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-01-28 02:45:35--  https://sampleswap.org/samples-ghost/VOCALS%20and%20SPOKEN%20WORD/Commercials%20and%20Radio/3207[kb]1941-newsreel-co_ed_warrior_women.aif.mp3\n",
            "Resolving sampleswap.org (sampleswap.org)... 69.167.149.24\n",
            "Connecting to sampleswap.org (sampleswap.org)|69.167.149.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185497 (181K) [audio/mpeg]\n",
            "Saving to: â€˜3207[kb]1941-newsreel-co_ed_warrior_women.aif.mp3.1â€™\n",
            "\n",
            "\r          3207[kb]1   0%[                    ]       0  --.-KB/s               \r3207[kb]1941-newsre 100%[===================>] 181.15K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-01-28 02:45:35 (1.69 MB/s) - â€˜3207[kb]1941-newsreel-co_ed_warrior_women.aif.mp3.1â€™ saved [185497/185497]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_sXWrLyq7v8O",
        "colab": {}
      },
      "source": [
        "#splitting audio file to 3s\n",
        "def trim(fn, dest='/content/temp'):\n",
        "  os.system(f'mkdir -p {dest} && cd {dest} && sox {FOLDER}/{fn} {fn} trim 0 3 : newfile : restart')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cd_BkNcx7v8W",
        "colab": {}
      },
      "source": [
        "trim('aud.wav', FOLDER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cmECPsYi7v8e",
        "colab": {}
      },
      "source": [
        "def createhead():\n",
        "  os.system(f\"echo 'fname,Accelerating_and_revving_and_vroom,Accordion,Acoustic_guitar,Applause,Bark,Bass_drum,Bass_guitar,Bathtub_(filling_or_washing),Bicycle_bell,Burping_and_eructation,Bus,Buzz,Car_passing_by,Cheering,Chewing_and_mastication,Child_speech_and_kid_speaking,Chink_and_clink,Chirp_and_tweet,Church_bell,Clapping,Computer_keyboard,Crackle,Cricket,Crowd,Cupboard_open_or_close,Cutlery_and_silverware,Dishes_and_pots_and_pans,Drawer_open_or_close,Drip,Electric_guitar,Fart,Female_singing,Female_speech_and_woman_speaking,Fill_(with_liquid),Finger_snapping,Frying_(food),Gasp,Glockenspiel,Gong,Gurgling,Harmonica,Hi-hat,Hiss,Keys_jangling,Knock,Male_singing,Male_speech_and_man_speaking,Marimba_and_xylophone,Mechanical_fan,Meow,Microwave_oven,Motorcycle,Printer,Purr,Race_car_and_auto_racing,Raindrop,Run,Scissors,Screaming,Shatter,Sigh,Sink_(filling_or_washing),Skateboard,Slam,Sneeze,Squeak,Stream,Strum,Tap,Tick-tock,Toilet_flush,Traffic_noise_and_roadway_noise,Trickle_and_dribble,Walk_and_footsteps,Water_tap_and_faucet,Waves_and_surf,Whispering,Writing,Yell,Zipper_(clothing)' > {LIST}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W1ek_ZV97v8i",
        "colab": {}
      },
      "source": [
        "def fntocsv(fn):\n",
        "  val = ',0'\n",
        "  os.system(f\"echo '{fn}{val*80}' >> {LIST}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e0N_o5OB7v8l",
        "colab": {}
      },
      "source": [
        "#create csv for all files in FOLDER\n",
        "createhead()\n",
        "for fn in os.listdir(FOLDER):\n",
        "  fntocsv(str(fn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liuY97weB2WK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(LIST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98uV0LRL86B3",
        "colab_type": "text"
      },
      "source": [
        "# Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVDvViiqM8q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "def read_audio(conf, pathname, trim_long_data):\n",
        "    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n",
        "    # trim silence\n",
        "    if 0 < len(y): # workaround: 0 length causes error\n",
        "        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n",
        "    # make it unified length to conf.samples\n",
        "    if len(y) > conf.samples: # long enough\n",
        "        if trim_long_data:\n",
        "            y = y[0:0+conf.samples]\n",
        "    else: # pad blank\n",
        "        padding = conf.samples - len(y)    # add padding at both ends\n",
        "        offset = padding // 2\n",
        "        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n",
        "    return y\n",
        "\n",
        "def audio_to_melspectrogram(conf, audio):\n",
        "    spectrogram = librosa.feature.melspectrogram(audio, \n",
        "                                                 sr=conf.sampling_rate,\n",
        "                                                 n_mels=conf.n_mels,\n",
        "                                                 hop_length=conf.hop_length,\n",
        "                                                 n_fft=conf.n_fft,\n",
        "                                                 fmin=conf.fmin,\n",
        "                                                 fmax=conf.fmax)\n",
        "    spectrogram = librosa.power_to_db(spectrogram)\n",
        "    spectrogram = spectrogram.astype(np.float32)\n",
        "    return spectrogram\n",
        "\n",
        "def show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n",
        "    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n",
        "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
        "                            fmin=conf.fmin, fmax=conf.fmax)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n",
        "    x = read_audio(conf, pathname, trim_long_data)\n",
        "    mels = audio_to_melspectrogram(conf, x)\n",
        "    if debug_display:\n",
        "        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n",
        "        show_melspectrogram(conf, mels)\n",
        "    return mels\n",
        "\n",
        "\n",
        "class conf:\n",
        "    # Preprocessing settings\n",
        "    sampling_rate = 44100\n",
        "    duration = 2\n",
        "    hop_length = 347*duration # to make time steps 128\n",
        "    fmin = 20\n",
        "    fmax = sampling_rate // 2\n",
        "    n_mels = 128\n",
        "    n_fft = n_mels * 20\n",
        "    samples = sampling_rate * duration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxm3BeatNjSC",
        "colab_type": "code",
        "outputId": "6a951027-4bae-45d2-dbc8-a31ab75a9528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "45d947b353e24e20ad85dbe6052119cc",
            "5ad8413f4b3741328c6d4f9ff1da3b59",
            "8f92a1d0c9c743d896816a3a7304c161",
            "9274f97b42b041d2810bb1e508dd723f",
            "168c1f7b53e04e7bbc785f709e80a88f",
            "05cbccfafd8445b394b55bf50bc575e5",
            "9bf4c9bb11be432197aedee9541698dc",
            "85fe45ba6c72416b8df9231cfc063e35"
          ]
        }
      },
      "source": [
        "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
        "    # Stack X as [X,X,X]\n",
        "    X = np.stack([X, X, X], axis=-1)\n",
        "\n",
        "    # Standardize\n",
        "    mean = mean or X.mean()\n",
        "    std = std or X.std()\n",
        "    Xstd = (X - mean) / (std + eps)\n",
        "    _min, _max = Xstd.min(), Xstd.max()\n",
        "    norm_max = norm_max or _max\n",
        "    norm_min = norm_min or _min\n",
        "    if (_max - _min) > eps:\n",
        "        # Scale to [0, 255]\n",
        "        V = Xstd\n",
        "        V[V < norm_min] = norm_min\n",
        "        V[V > norm_max] = norm_max\n",
        "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
        "        V = V.astype(np.uint8)\n",
        "    else:\n",
        "        # Just zero\n",
        "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
        "    return V\n",
        "\n",
        "def convert_wav_to_image(df, source, img_dest=''):\n",
        "    X = []\n",
        "    # for row in df.iterrows():\n",
        "    for i, row in tqdm_notebook(df.iterrows()):\n",
        "        x = read_as_melspectrogram(conf, source/str(row.fname), trim_long_data=False)\n",
        "        x_color = mono_to_color(x)\n",
        "        X.append(x_color)\n",
        "    return X\n",
        "\n",
        "Xval = convert_wav_to_image(df, source=SOURCE)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45d947b353e24e20ad85dbe6052119cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO7JKC2hTLGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.vision.data import *\n",
        "import random\n",
        "\n",
        "CUR_X_FILES, CUR_X = list(df.fname.values), Xval\n",
        "\n",
        "def open_fat2019_image(fn, convert_mode, after_open)->Image:\n",
        "    print(fn, \"FN\")\n",
        "    # open\n",
        "    idx = CUR_X_FILES.index(fn.split('/')[-1])\n",
        "    # idx = int(fn.split('/')[-1])\n",
        "    x = PIL.Image.fromarray(CUR_X[idx])\n",
        "    # crop\n",
        "    time_dim, base_dim = x.size\n",
        "    crop_x = random.randint(0, time_dim - base_dim)\n",
        "    x = x.crop([crop_x, 0, crop_x+base_dim, base_dim])    \n",
        "    # standardize\n",
        "    return Image(pil2tensor(x, np.float32).div_(255))\n",
        "\n",
        "vision.data.open_image = open_fat2019_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_vemFGtfmtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _one_sample_positive_class_precisions(scores, truth):\n",
        "    \"\"\"Calculate precisions for each true class for a single sample.\n",
        "\n",
        "    Args:\n",
        "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
        "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
        "\n",
        "    Returns:\n",
        "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
        "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
        "        classes.\n",
        "    \"\"\"\n",
        "    num_classes = scores.shape[0]\n",
        "    pos_class_indices = np.flatnonzero(truth > 0)\n",
        "    # Only calculate precisions if there are some true classes.\n",
        "    if not len(pos_class_indices):\n",
        "        return pos_class_indices, np.zeros(0)\n",
        "    # Retrieval list of classes for this sample.\n",
        "    retrieved_classes = np.argsort(scores)[::-1]\n",
        "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
        "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
        "    class_rankings[retrieved_classes] = range(num_classes)\n",
        "    # Which of these is a true label?\n",
        "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
        "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
        "    # Num hits for every truncated retrieval list.\n",
        "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
        "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
        "    precision_at_hits = (\n",
        "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
        "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
        "    return pos_class_indices, precision_at_hits\n",
        "\n",
        "\n",
        "def calculate_per_class_lwlrap(truth, scores):\n",
        "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
        "\n",
        "    Arguments:\n",
        "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
        "        of presence of that class in that sample.\n",
        "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
        "        test's real-valued score for each class for each sample.\n",
        "\n",
        "    Returns:\n",
        "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
        "        class.\n",
        "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
        "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
        "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
        "    \"\"\"\n",
        "    assert truth.shape == scores.shape\n",
        "    num_samples, num_classes = scores.shape\n",
        "    # Space to store a distinct precision value for each class on each sample.\n",
        "    # Only the classes that are true for each sample will be filled in.\n",
        "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
        "    for sample_num in range(num_samples):\n",
        "        pos_class_indices, precision_at_hits = (\n",
        "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
        "                                                  truth[sample_num, :]))\n",
        "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
        "            precision_at_hits)\n",
        "    labels_per_class = np.sum(truth > 0, axis=0)\n",
        "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
        "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
        "    # a particular class.\n",
        "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
        "                        np.maximum(1, labels_per_class))\n",
        "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
        "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
        "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
        "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
        "    return per_class_lwlrap, weight_per_class\n",
        "\n",
        "\n",
        "# Wrapper for fast.ai library\n",
        "def lwlrap(scores, truth, **kwargs):\n",
        "    score, weight = calculate_per_class_lwlrap(to_np(truth), to_np(scores))\n",
        "    return torch.Tensor([(score * weight).sum()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYct7VKGUJ-z",
        "colab_type": "code",
        "outputId": "70624282-40ac-431e-f906-02b827768227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        }
      },
      "source": [
        "CUR_X_FILES, CUR_X = list(df.fname.values), Xval\n",
        "\n",
        "test = ImageList.from_csv(ROOT, LIST, folder=FOLDER)\n",
        "learn = load_learner(ROOT, CATMODEL, test=test)\n",
        "preds, _ = learn.TTA(ds_type=DatasetType.Test)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/aud/aud.wav FN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.loss.BCEWithLogitsLoss' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torchvision.models.resnet.BasicBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'fastai.layers.AdaptiveConcatPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveAvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveMaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'fastai.layers.Flatten' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n",
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n",
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n",
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n",
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n",
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n",
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n",
            "/content/aud/aud.wav FN\n",
            "/content/aud/aud2.wav FN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWG93BkZpAE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[learn.data.classes] = preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1icLZKfqyeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_res=df.drop(columns=['fname'])\n",
        "for c in df_res.columns:\n",
        "  df_res[c] = pd.to_numeric(df_res[c])\n",
        "df_res['Sound'] = df_res.idxmax(axis=1)\n",
        "df_res=pd.concat([df['fname'],df_res['Sound']], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR1IqcboqcfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_res.to_csv('category.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WStEFnt_sTsq",
        "colab_type": "code",
        "outputId": "96ffb4d0-1e2b-4b16-9813-b12611aeefb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "df_res.head()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fname</th>\n",
              "      <th>Sound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aud.wav</td>\n",
              "      <td>Male_speech_and_man_speaking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aud2.wav</td>\n",
              "      <td>Male_singing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      fname                         Sound\n",
              "0   aud.wav  Male_speech_and_man_speaking\n",
              "1  aud2.wav                  Male_singing"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmRn-02co8L7",
        "colab_type": "text"
      },
      "source": [
        "# DeepSpeech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o7MR6ATk_Hq",
        "colab_type": "code",
        "outputId": "81feef8c-95a9-4c13-c6c9-cc12dfd0dae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install deepspeech > /dev/null\n",
        "# Download pre-trained English model and extract\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/deepspeech-0.6.1-models.tar.gz\n",
        "!tar xvf deepspeech-0.6.1-models.tar.gz\n",
        "# Download example audio files\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/audio-0.6.1.tar.gz > /dev/null\n",
        "!tar xvf audio-0.6.1.tar.gz\n",
        "!apt-get -qq install sox libsox-fmt-mp3"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeech\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/8b/82512dbb0a70a6b3ce11d3ee9165a8a2d5830f9e42f631b640685e5045dc/deepspeech-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (9.6MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.6MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech) (1.17.5)\n",
            "Installing collected packages: deepspeech\n",
            "Successfully installed deepspeech-0.6.1\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   620    0   620    0     0   2649      0 --:--:-- --:--:-- --:--:--  2638\n",
            "100 1172M  100 1172M    0     0  73.8M      0  0:00:15  0:00:15 --:--:-- 79.5M\n",
            "._deepspeech-0.6.1-models\n",
            "deepspeech-0.6.1-models/\n",
            "deepspeech-0.6.1-models/._lm.binary\n",
            "deepspeech-0.6.1-models/lm.binary\n",
            "deepspeech-0.6.1-models/._output_graph.pbmm\n",
            "deepspeech-0.6.1-models/output_graph.pbmm\n",
            "deepspeech-0.6.1-models/._output_graph.pb\n",
            "deepspeech-0.6.1-models/output_graph.pb\n",
            "deepspeech-0.6.1-models/._trie\n",
            "deepspeech-0.6.1-models/trie\n",
            "deepspeech-0.6.1-models/output_graph.tflite\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   608    0   608    0     0   2412      0 --:--:-- --:--:-- --:--:--  2412\n",
            "100  192k  100  192k    0     0   376k      0 --:--:-- --:--:-- --:--:-- 1013k\n",
            "audio/\n",
            "audio/2830-3980-0043.wav\n",
            "audio/Attribution.txt\n",
            "audio/4507-16021-0012.wav\n",
            "audio/8455-210777-0068.wav\n",
            "audio/License.txt\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3\n",
            "Suggested packages:\n",
            "  file libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3 sox\n",
            "0 upgraded, 8 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 760 kB of archives.\n",
            "After this operation, 6,715 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.3 [184 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.3 [68.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
            "Fetched 760 kB in 1s (850 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 145674 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../2-libmagic-mgc_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../3-libmagic1_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../4-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../6-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../7-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTr0OoIyk_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transcribe(fn):\n",
        "  # Transcribe an audio file in audio folder\n",
        "  os.system(f'sox {FOLDER}/{fn} --bits 16 --channels 1 --rate 16000 --encoding signed-integer --endian little --compression 0.0 --no-dither .temp.wav')\n",
        "  os.system(f\"deepspeech --model deepspeech-0.6.1-models/output_graph.pbmm --lm deepspeech-0.6.1-models/lm.binary --trie deepspeech-0.6.1-models/trie --audio .temp.wav > .transcript\")\n",
        "  with open('.transcript', 'r') as file:\n",
        "    data = file.read().rstrip('\\n')\n",
        "  if data:\n",
        "    os.remove('.temp.wav')\n",
        "    os.remove('.transcript')\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeYFAFhCuG5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a021b616-055c-46e3-a8f4-75710fe3afd0"
      },
      "source": [
        "speech = []\n",
        "for fn in os.listdir(FOLDER):\n",
        "  words = transcribe(fn)\n",
        "  speech.append(words)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/head.csv')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG8NhZ7bKbFG",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYTVJDl_KI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "human_speech = ['Child_speech_and_kid_speaking', 'Female_singing', 'Female_speech_and_woman_speaking', 'Male_singing', 'Male_speech_and_man_speaking', 'Whispering', 'Yell']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCabZRy8-bOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "description = []\n",
        "for i, label in df_res['Sound']:\n",
        "  if label in human_speech:\n",
        "    val = f\"{label}: {speech[i]}\"\n",
        "  else:\n",
        "    val = str(label)\n",
        "  description.append(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AnhKa_bBJ7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = ' - '.join(description)\n",
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}