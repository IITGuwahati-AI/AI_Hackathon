{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "export.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p4cwNU45Dcu",
        "colab_type": "text"
      },
      "source": [
        "# Media Description Generator\n",
        "\n",
        "ResNet18 architecture is used with ImageNet pretrained model, we also tried with Pretrained='False',but given the time constraints,we could not get a good convergence (see README), and pretrained model gave better result and converged early. \n",
        "\n",
        "Summary:\n",
        "- Converting audio to 2D image spectrogram, so that we use  CNN classifier using vision based deep learing techniques.\n",
        "- Use transfer learning by using ImageNet weight initialization.\n",
        "- We are using FastAI library to build this multi class clasifier and then we call our next pretrained model **DeepSpeech** for caption generation.\n",
        "- Implementation of LwLARP taken from [Dan Ellis](https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "5nHE8PdI5Dcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "import IPython\n",
        "import IPython.display\n",
        "import PIL\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuP67mQihkNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.vision import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xbp3u5ShmiK",
        "colab_type": "code",
        "outputId": "706337a9-e38f-4d4f-d584-e10151390018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "#downloading dataset\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en;q=0.9,en-US;q=0.8,hi;q=0.7\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10700/503808/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1580296888&Signature=JYVxUjlBN08F4EPTgrmhrTY2swSqc60S4V2IDfD5pQMf8rez1eOb9lGJn2o04JEIF1q9hWJXCeCD2zm9kC6tn4vivrEizrJ0Gs8ZFx%2BNDMagbOBJ6B6tatYHOodncAmHldL3eVcc86Q7VdRn55goP0%2FUBadwLQj1CKCgPI58ZK1hBKGtloOnSRo2ZTbrmVvKp2v7hVnrkAO8BmGJoWHMAbJU0JezEj2sJTN%2Bfxcs9QCAPJINJ5aDtlfhoNS8nLkmD0H5L1BhnAtT87n51%2BUpE81A2du07O7EEEw6K8OhrtZT5kqAPopgA20Mw5%2B7nSUJZFqaPIA%2FTeFEaQToIoKSZA%3D%3D&response-content-disposition=attachment%3B+filename%3Dfreesound-audio-tagging-2019.zip\" -O \"freesound-audio-tagging-2019.zip\" -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-27 09:34:29--  https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10700/503808/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1580296888&Signature=JYVxUjlBN08F4EPTgrmhrTY2swSqc60S4V2IDfD5pQMf8rez1eOb9lGJn2o04JEIF1q9hWJXCeCD2zm9kC6tn4vivrEizrJ0Gs8ZFx%2BNDMagbOBJ6B6tatYHOodncAmHldL3eVcc86Q7VdRn55goP0%2FUBadwLQj1CKCgPI58ZK1hBKGtloOnSRo2ZTbrmVvKp2v7hVnrkAO8BmGJoWHMAbJU0JezEj2sJTN%2Bfxcs9QCAPJINJ5aDtlfhoNS8nLkmD0H5L1BhnAtT87n51%2BUpE81A2du07O7EEEw6K8OhrtZT5kqAPopgA20Mw5%2B7nSUJZFqaPIA%2FTeFEaQToIoKSZA%3D%3D&response-content-disposition=attachment%3B+filename%3Dfreesound-audio-tagging-2019.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.127.128, 2a00:1450:4013:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.127.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26154728032 (24G) [application/zip]\n",
            "Saving to: ‘freesound-audio-tagging-2019.zip’\n",
            "\n",
            "freesound-audio-tag 100%[===================>]  24.36G  35.3MB/s    in 6m 57s  \n",
            "\n",
            "2020-01-27 09:41:27 (59.8 MB/s) - ‘freesound-audio-tagging-2019.zip’ saved [26154728032/26154728032]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw8z-zOell1q",
        "colab_type": "code",
        "outputId": "56b26fd1-73d0-4f48-d9ea-dafb7e7f4f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!unzip \"freesound-audio-tagging-2019.zip\" "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  freesound-audio-tagging-2019.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.zip                \n",
            "  inflating: train_curated.csv       \n",
            "  inflating: train_curated.zip       \n",
            "  inflating: train_noisy.csv         \n",
            "  inflating: train_noisy.zip         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEnRFAffhfvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm \"freesound-audio-tagging-2019.zip\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjVy8ldnjkUf",
        "colab_type": "code",
        "outputId": "6e50eb4f-274b-451d-b4a9-10eb5dc535ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-sNXSgi5Dc2",
        "colab_type": "text"
      },
      "source": [
        "## File/folder definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "phEEDIRY5Dc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA = Path('/content')\n",
        "CSV_TRN_CURATED = DATA/'train_curated.csv'\n",
        "CSV_SKELETON = DATA/'skeleton.csv'\n",
        "TRN_CURATED = DATA/'train_curated'\n",
        "TEST = DATA/'test'\n",
        "WORK = Path('work')\n",
        "IMG_TRN_CURATED = WORK/'image/trn_curated'\n",
        "IMG_TEST = WORK/'image/test'\n",
        "for folder in [WORK, IMG_TRN_CURATED, IMG_TEST]: \n",
        "    Path(folder).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "df = pd.read_csv(CSV_TRN_CURATED)\n",
        "test_df = pd.read_csv(CSV_SKELETON)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlwulWWA5Dc6",
        "colab_type": "text"
      },
      "source": [
        "## Audio conversion to 2D spectrogram\n",
        "\n",
        "Spectrogram creation inspired from https://github.com/daisukelab/ml-sound-classifier\n",
        "- Size of each file will be 128 x L, L is audio seconds x 128; `[128, 256]  provided sound is 2s long.\n",
        "- We now convert into spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gjxH79r25Dc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "def read_audio(conf, pathname, trim_long_data):\n",
        "    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n",
        "    # trim silence\n",
        "    if 0 < len(y): # workaround: 0 length causes error\n",
        "        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n",
        "    # make it unified length to conf.samples\n",
        "    if len(y) > conf.samples: # long enough\n",
        "        if trim_long_data:\n",
        "            y = y[0:0+conf.samples]\n",
        "    else: # pad blank\n",
        "        padding = conf.samples - len(y)    # add padding at both ends\n",
        "        offset = padding // 2\n",
        "        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n",
        "    return y\n",
        "\n",
        "def audio_to_melspectrogram(conf, audio):\n",
        "    spectrogram = librosa.feature.melspectrogram(audio, \n",
        "                                                 sr=conf.sampling_rate,\n",
        "                                                 n_mels=conf.n_mels,\n",
        "                                                 hop_length=conf.hop_length,\n",
        "                                                 n_fft=conf.n_fft,\n",
        "                                                 fmin=conf.fmin,\n",
        "                                                 fmax=conf.fmax)\n",
        "    spectrogram = librosa.power_to_db(spectrogram)\n",
        "    spectrogram = spectrogram.astype(np.float32)\n",
        "    return spectrogram\n",
        "\n",
        "def show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n",
        "    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n",
        "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
        "                            fmin=conf.fmin, fmax=conf.fmax)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n",
        "    x = read_audio(conf, pathname, trim_long_data)\n",
        "    mels = audio_to_melspectrogram(conf, x)\n",
        "    if debug_display:\n",
        "        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n",
        "        show_melspectrogram(conf, mels)\n",
        "    return mels\n",
        "\n",
        "\n",
        "class conf:\n",
        "    # Preprocessing settings\n",
        "    sampling_rate = 44100\n",
        "    duration = 2\n",
        "    hop_length = 347*duration # to make time steps 128\n",
        "    fmin = 20\n",
        "    fmax = sampling_rate // 2\n",
        "    n_mels = 128\n",
        "    n_fft = n_mels * 20\n",
        "    samples = sampling_rate * duration\n",
        "\n",
        "# example\n",
        "#x = read_as_melspectrogram(conf, TRN_CURATED/'0006ae4e.wav', trim_long_data=False, debug_display=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZFidnQW5DdA",
        "colab_type": "text"
      },
      "source": [
        "## Converting 2D spectrogram data to three channel image files\n",
        "\n",
        " So that we can use normal CNN for classification\n",
        "\n",
        "\n",
        "\n",
        "We are keeping data on memory instead of saving on disk for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F3Nzr4dZ5DdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
        "    # Stack X as [X,X,X]\n",
        "    X = np.stack([X, X, X], axis=-1)\n",
        "\n",
        "    # Standardize\n",
        "    mean = mean or X.mean()\n",
        "    std = std or X.std()\n",
        "    Xstd = (X - mean) / (std + eps)\n",
        "    _min, _max = Xstd.min(), Xstd.max()\n",
        "    norm_max = norm_max or _max\n",
        "    norm_min = norm_min or _min\n",
        "    if (_max - _min) > eps:\n",
        "        # Scale to [0, 255]\n",
        "        V = Xstd\n",
        "        V[V < norm_min] = norm_min\n",
        "        V[V > norm_max] = norm_max\n",
        "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
        "        V = V.astype(np.uint8)\n",
        "    else:\n",
        "        # Just zero\n",
        "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
        "    return V\n",
        "\n",
        "def convert_wav_to_image(df, source, img_dest):\n",
        "    X = []\n",
        "    for i, row in tqdm_notebook(df.iterrows()):\n",
        "        x = read_as_melspectrogram(conf, source/str(row.fname), trim_long_data=False)\n",
        "        x_color = mono_to_color(x)\n",
        "        X.append(x_color)\n",
        "    return X\n",
        "\n",
        "X_train = convert_wav_to_image(df, source=TRN_CURATED, img_dest=IMG_TRN_CURATED)\n",
        "X_test = convert_wav_to_image(test_df, source=TEST, img_dest=IMG_TEST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k3RtTAQ5DdF",
        "colab_type": "text"
      },
      "source": [
        "## Specifying `open_image` for fast.ai library to load data from memory\n",
        " Also using random cropping of 1 sec, for data augmentation similar to images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fTSXBiNR5DdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.vision.data import *\n",
        "import random\n",
        "\n",
        "CUR_X_FILES, CUR_X = list(df.fname.values), X_train\n",
        "\n",
        "def open_fat2019_image(fn, convert_mode, after_open)->Image:\n",
        "    # open\n",
        "    idx = CUR_X_FILES.index(fn.split('/')[-1])\n",
        "    x = PIL.Image.fromarray(CUR_X[idx])\n",
        "    # crop\n",
        "    time_dim, base_dim = x.size\n",
        "    crop_x = random.randint(0, time_dim - base_dim)\n",
        "    x = x.crop([crop_x, 0, crop_x+base_dim, base_dim])    \n",
        "    # standardize\n",
        "    return Image(pil2tensor(x, np.float32).div_(255))\n",
        "\n",
        "vision.data.open_image = open_fat2019_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsqrEBnz5DdI",
        "colab_type": "text"
      },
      "source": [
        "## Code for multi-label classification \n",
        "\n",
        "*Implementation of LwLARP taken from [Dan Ellis](https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8).*\n",
        "\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mVdbwwD85DdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
        "def _one_sample_positive_class_precisions(scores, truth):\n",
        "    \"\"\"Calculate precisions for each true class for a single sample.\n",
        "\n",
        "    Args:\n",
        "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
        "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
        "\n",
        "    Returns:\n",
        "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
        "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
        "        classes.\n",
        "    \"\"\"\n",
        "    num_classes = scores.shape[0]\n",
        "    pos_class_indices = np.flatnonzero(truth > 0)\n",
        "    # Only calculate precisions if there are some true classes.\n",
        "    if not len(pos_class_indices):\n",
        "        return pos_class_indices, np.zeros(0)\n",
        "    # Retrieval list of classes for this sample.\n",
        "    retrieved_classes = np.argsort(scores)[::-1]\n",
        "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
        "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
        "    class_rankings[retrieved_classes] = range(num_classes)\n",
        "    # Which of these is a true label?\n",
        "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
        "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
        "    # Num hits for every truncated retrieval list.\n",
        "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
        "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
        "    precision_at_hits = (\n",
        "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
        "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
        "    return pos_class_indices, precision_at_hits\n",
        "\n",
        "\n",
        "def calculate_per_class_lwlrap(truth, scores):\n",
        "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
        "\n",
        "    Arguments:\n",
        "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
        "        of presence of that class in that sample.\n",
        "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
        "        test's real-valued score for each class for each sample.\n",
        "\n",
        "    Returns:\n",
        "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
        "        class.\n",
        "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
        "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
        "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
        "    \"\"\"\n",
        "    assert truth.shape == scores.shape\n",
        "    num_samples, num_classes = scores.shape\n",
        "    # Space to store a distinct precision value for each class on each sample.\n",
        "    # Only the classes that are true for each sample will be filled in.\n",
        "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
        "    for sample_num in range(num_samples):\n",
        "        pos_class_indices, precision_at_hits = (\n",
        "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
        "                                                  truth[sample_num, :]))\n",
        "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
        "            precision_at_hits)\n",
        "    labels_per_class = np.sum(truth > 0, axis=0)\n",
        "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
        "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
        "    # a particular class.\n",
        "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
        "                        np.maximum(1, labels_per_class))\n",
        "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
        "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
        "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
        "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
        "    return per_class_lwlrap, weight_per_class\n",
        "\n",
        "\n",
        "# Wrapper for fast.ai library\n",
        "def lwlrap(scores, truth, **kwargs):\n",
        "    score, weight = calculate_per_class_lwlrap(to_np(truth), to_np(scores))\n",
        "    return torch.Tensor([(score * weight).sum()])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnSqnStbATo6",
        "colab_type": "text"
      },
      "source": [
        "###Transforms and Creating Learner for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hedqJael5DdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfms = get_transforms(do_flip=True, max_rotate=0, max_lighting=0.1, max_zoom=0, max_warp=0.)\n",
        "src = (ImageList.from_csv(WORK/'image', Path('../../')/CSV_TRN_CURATED, folder='trn_curated')\n",
        "       .split_by_rand_pct(0.2)\n",
        "       .label_from_df(label_delim=',')\n",
        ")\n",
        "data = (src.transform(tfms, size=128)\n",
        "        .databunch(bs=64).normalize(imagenet_stats)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PWbgFk3h5DdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.show_batch(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yMwzDOqe5DdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = cnn_learner(data, models.resnet18, pretrained=True, metrics=[lwlrap])\n",
        "learn.unfreeze()\n",
        "\n",
        "learn.lr_find(); learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SvDePaPY5DdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(3, 1e-1)\n",
        "learn.fit_one_cycle(7, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FMgCucbh5DdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find(); learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NcR6GDkm5Dda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(15,2e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "k5-D1IM35Dde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find(); learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uM8AG49f5Ddl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(25, slice(1e-3, 2e-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5pMo2nNu5Ddn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(5, slice(1e-4, 2e-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTwbnQISAhwV",
        "colab_type": "text"
      },
      "source": [
        "# Saving the model for later inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6d6kCj7H5Ddv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('stage-2')\n",
        "learn.export()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}